# Lightweight Fine-Tuning Project
TODO: In this cell, describe your choices for each of the following

* PEFT technique: 
* Model: 
* Evaluation approach: 
* Fine-tuning dataset: 
## Loading and Evaluating a Foundation Model

TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset.
!pip install torch transformers datasets peft accelerate 
!pip install huggingface_hub scikit-learn
!pip install evaluate
!pip install scikit-learn
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, PeftModel, AutoPeftModelForSequenceClassification, TaskType
import numpy as np
import torch
import os
import shutil
import sys
import time
import traceback
import json
from datasets import DatasetDict, load_metric

# aka big bang
planck0 = time.time()

def p(text, width=80):
    print("\n"*3+"="*width+"\n"+text.upper().center(width)+"\n"+"="*width)
    
def secToHuman(elapsed_time):
    hours, rem = divmod(elapsed_time, 3600)
    minutes, seconds = divmod(rem, 60)
    # return hours, minutes, seconds
    return f"{int(hours):02}:{int(minutes):02}:{seconds:.2f}"    

# variables
CHECKPOINTS = "./checkpoints"
PEFT_MODEL = "./peft_model"

resultset = []

label_names = ["not spam", "spam"]
id2label = {idx: label for idx, label in enumerate(label_names)}
label2id = {label: idx for idx, label in enumerate(label_names)}
model = None
peft_model = None
tokenizer = None
tokenized_ds = {} 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

###############################################################################
# Loading and Evaluating a Foundation Model
###############################################################################
split = ['train', 'test']

raw_dataset = load_dataset("sms_spam")
full_dataset = raw_dataset['train'].train_test_split(test_size=0.2, seed=42, shuffle=True)

# dataset =  DatasetDict({"train": full_dataset["train"].shuffle(seed=42).select(range(1000)),  # Keep only 1000 samples
#             "test":  full_dataset["test"].shuffle(seed=42).select(range(200))  # Keep only 200 samples
#             })
dataset =  DatasetDict({"train": full_dataset["train"].shuffle(seed=42),  
            "test":  full_dataset["test"].shuffle(seed=42)
            })

print("Dataset loaded.")

def gpt2_compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):  # Ensure it's not a dictionary
        logits = logits[0]
    predictions = np.argmax(logits, axis=1)
    return {"accuracy": (predictions == labels).mean()}

def tokenize_fn(examples):
    global tokenizer
    return tokenizer(examples["sms"], padding="max_length", truncation=True, max_length=256)


def evaluate_gpt2_model(dataset, with_train=False):
    global tokenizer
    # Load GPT-2 tokenizer and model
    # from transformers import BitsAndBytesConfig
    # bnb_config = BitsAndBytesConfig(load_in_8bit=True)  # 8-bit quantization

    model_name = "gpt2"
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, num_labels=2, id2label=id2label, label2id=label2id,
    #     quantization_config=bnb_config
    ).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # GPT-2 doesn't have a padding token, so use eos_token and set padding_side to left
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # Ensure left padding for GPT-2

    for s in split:
        tokenized_ds[s] = dataset[s].map(tokenize_fn, batched=True)

    tokenized_ds["train"] = tokenized_ds["train"].map(
        lambda e: {'labels': e['label']},  
        batched=True,
        remove_columns=['label']
    )
    tokenized_ds["test"] = tokenized_ds["test"].map(
        lambda e: {'labels': e['label']},  
        batched=True,
        remove_columns=['label']
    )

    tokenized_ds["train"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    tokenized_ds["test"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

    print("===========================================================")
    print(tokenized_ds["train"].column_names)
    print("===========================================================")


    ###############################################################################
    # Load model and freeze base parameters
    ###############################################################################

    model.config.pad_token_id = tokenizer.pad_token_id
    model.resize_token_embeddings(len(tokenizer))  # Adjust embedding size for new tokens

    for name, param in model.named_parameters():
        if "score" not in name:  # Keep classification head trainable
            param.requires_grad = True

    print(model)

    for name, param in model.named_parameters():
        print(name, param.requires_grad)


    trainer = Trainer(
        model=model,
        args=TrainingArguments(
            output_dir=CHECKPOINTS, 
            resume_from_checkpoint=True,
            learning_rate=2e-5, 
            per_device_train_batch_size=16, 
            per_device_eval_batch_size=16, 
            num_train_epochs=2, 
            weight_decay=0.01, 
            evaluation_strategy="epoch",
            save_strategy="epoch", 
            metric_for_best_model="accuracy",  # Change from "eval_loss" to "accuracy"
            load_best_model_at_end=True, 
        ),
        train_dataset=tokenized_ds["train"],
        eval_dataset=tokenized_ds["test"], 
        tokenizer=tokenizer,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt"),
        compute_metrics=gpt2_compute_metrics, 
    )

    metrics = trainer.evaluate()
    result = f"GPT2 Evaluation metrics before everything: {metrics}"
    if (with_train):
        trainer.train()
        metrics = trainer.evaluate()
        result+=f"\nEvaluation metrics after gpt2 train: {metrics}"
    return model, result
start_time = time.time()

model, result = evaluate_gpt2_model(dataset, True)

end_time = time.time()
elapsed_time = secToHuman(end_time - start_time)
r = ["Run GPT2 Evaluation and Traing", elapsed_time, result]
resultset.append(r)
print(f"{r}")
start_time = time.time()

model, result = evaluate_gpt2_model(dataset, False)

end_time = time.time()
elapsed_time = secToHuman(end_time - start_time)
r = ["Run GPT2 Evaluation", elapsed_time, result]
resultset.append(r)
print(f"{r}")

## Performing Parameter-Efficient Fine-Tuning

TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.
###############################################################################
# Performing Parameter-Efficient Fine-Tuning
###############################################################################
# tokenizer = AutoTokenizer.from_pretrained(GPT2_FINETUNED_MODEL)
# model = AutoModelForSequenceClassification.from_pretrained(GPT2_FINETUNED_MODEL, ignore_mismatched_sizes=True).to(device)
torch.cuda.empty_cache()


def lora_compute_metrics(eval_pred):
    logits, labels = eval_pred
    
    print("Logits Type:", type(logits))
    print("Logits Shape:", np.array(logits, dtype=object).shape)
    print("Labels Type:", type(labels))
    print("Labels Shape:", np.array(labels, dtype=object).shape)

    # Extract logits if they are inside a tuple
    if isinstance(logits, tuple):
        logits = logits[0]  # Take the first element of the tuple

    # Ensure logits is a NumPy array
    logits = np.array(logits)

    # Ensure labels is a NumPy array
    labels = np.array(labels)

    # Compute predictions
    predictions = np.argmax(logits, axis=-1)

    accuracy = load_metric("accuracy", trust_remote_code=True) 
    acc = accuracy.compute(predictions=predictions, references=labels)["accuracy"]

    print("Computed Accuracy:", acc)
    return {"eval_accuracy": acc}


start_time = time.time()
result = []

tokenized_ds = dataset.map(
    lambda x: tokenizer(x["sms"], padding="max_length", truncation=True, max_length=512),
    batched=True
)
tokenized_ds = tokenized_ds.rename_columns({"label": "labels"})
tokenized_ds["train"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
tokenized_ds["test"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])


lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, bias="none", task_type=TaskType.SEQ_CLS)
peft_model = get_peft_model(model, lora_config)
peft_model.print_trainable_parameters()
peft_model.to(device)

peft_model.save_pretrained(PEFT_MODEL)
print("peft model saved")


trainer = Trainer(
    model=peft_model,  # Make sure to pass the PEFT model here
    args=TrainingArguments(
        output_dir=CHECKPOINTS,
        resume_from_checkpoint=True,
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        logging_steps=1,
        weight_decay=0.01,
        evaluation_strategy="epoch",
        metric_for_best_model="eval_accuracy",  
        save_strategy="epoch",
        load_best_model_at_end=True,
        label_names=["labels"],
    ),
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt"),
    compute_metrics=lora_compute_metrics,
)
trainer.compute_metrics=lora_compute_metrics


from datasets import DatasetDict, load_metric

# Run evaluation manually
eval_output = trainer.predict(tokenized_ds["test"])
logits = eval_output.predictions
labels = eval_output.label_ids  
metrics = trainer.evaluate()
result.append(f"Lora Evaluation metrics before training: {metrics}")



print(tokenized_ds["test"].column_names)  # Should include "labels"
trainer.train() # resume_from_checkpoint=CHECKPOINTS+"/checkpoint-last")
metrics = trainer.evaluate()
result.append(f"Lora Evaluation metrics after training: {metrics}")


end_time = time.time()
elapsed_time = secToHuman(end_time - start_time)
r = ["Run Lora Evaluation and Traing", elapsed_time, result]
resultset.append(r)
print(f"{r}")

## Performing Inference with a PEFT Model

TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning.


###############################################################################
# Performing Inference with a PEFT Model
###############################################################################

def hf_compute_metrics(eval_pred):
    logits, labels = eval_pred

    if isinstance(logits, tuple):
        logits = logits[0]  # Extract logits array if it's a tuple

    predictions = np.argmax(logits, axis=-1)

    accuracy = load_metric("accuracy", trust_remote_code=True) 
    acc = accuracy.compute(predictions=predictions, references=labels)["accuracy"]

    return {"accuracy": acc}

start_time = time.time()
result = [] 
NUM_LABELS = 2

model = AutoModelForSequenceClassification.from_pretrained(PEFT_MODEL, ignore_mismatched_sizes=True).to(device)
print("peft model loaded")

model.to(device)
model.config.pad_token_id = tokenizer.pad_token_id

trainer = Trainer(
    model=peft_model,  # PEFT model
    args=TrainingArguments(
        output_dir=CHECKPOINTS, 
        resume_from_checkpoint=True,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss", 
    ),
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["test"],
    tokenizer=tokenizer,
    compute_metrics=hf_compute_metrics,
)

# Evaluate the fine-tuned model on the test set
hf_results = trainer.evaluate()
result.append(f"Hugging Face Evaluation metrics: {hf_results}")

end_time = time.time()
elapsed_time = secToHuman(end_time - start_time)
r = ["Run Hugging Face Evaluation", elapsed_time, result]
resultset.append(r)
print(f"{r}")




from datetime import datetime

big_crunch = time.time()
elapsed_time = secToHuman(big_crunch - planck0)
start_d = datetime.fromtimestamp(planck0 / 1000).strftime('%Y-%m-%d %H:%M:%S')
end_d = datetime.fromtimestamp(big_crunch / 1000).strftime('%Y-%m-%d %H:%M:%S')

r = {"Runtime Summary",  elapsed_time, ""}
resultset.append(r)

import pandas as pd

# Convert to DataFrame
df = pd.DataFrame(resultset, columns=["Task", "Elapsed Time", "Result"])
df_styled = df.style.set_properties(**{"text-align": "left", "white-space": "pre-wrap"})  # Preserve formatting

col_space_dict = {"Task": 30, "Elapsed Time": 15, "Result": 15}

# Display in Jupyter Notebook
display(df_styled)

# Write to a fixed-width formatted text file
output_file = "resultset.txt"
text = df.to_string(index=False, col_space=col_space_dict, justify="left")
with open(output_file, "w") as f:
     f.write(text)  # Fixed-width columns

print("\n\n\nSo Long, and Thanks for All the Fish\n\n")
